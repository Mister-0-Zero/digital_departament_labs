# Отчёт по лабораторной работе: Метод опорных векторов (SVM)

## Цель работы

В ходе лабораторной работы я изучил метод опорных векторов (SVM) и применил его для решения задачи классификации двумерных наборов данных. Основными этапами были:

1. Реализация и тестирование линейного SVM.
2. Реализация и тестирование SVM с гауссовским ядром (RBF).
3. Подбор оптимальных гиперпараметров `C` и `σ` через кросс-валидацию.

## Используемые файлы

- **lab\_intelligent\_systems\_SVM.ipynb** — основной ноутбук.
- **ex3data1.mat**, **ex3data2.mat**, **ex3data3.mat** — три набора данных.

## Ход работы

### 1. Визуализация данных

- Написал функцию `plotData(X, y)`, которая при помощи `matplotlib.scatter` отображает два класса разными маркерами.
- Проверил работу на первом наборе `ex3data1.mat` — точки чётко разделяются.

### 2. Линейное ядро

- Реализовал функцию `linearKernel(x1, x2) = x1.T @ x2` и использовал её для проверки скалярных произведений.
- Обрнул обёртку `svmTrain(X, y, C, "linear")` над `sklearn.SVC(kernel='linear')`.
- Обучил модель на `ex3data1.mat` с `C=1`, получил прямую границу.
- Визуализировал прямую в функции `visualizeBoundaryLinear`, вычисляя `y = -(w[0]*x + b)/w[1]`.

### 3. Гауссовское ядро (RBF)

- Написал функцию
  ```python
  def gaussianKernel(x1, x2, sigma):
      diff = x1.flatten() - x2.flatten()
      return np.exp(-np.dot(diff, diff) / (2 * sigma**2))
  ```
- Дообернул `svmTrain` для `kernel='precomputed'`, строя матрицу Грама через `gaussianKernel`.
- Обучил модель с RBF-ядром на `ex3data2.mat`, визуализировал границу через `visualizeBoundary`:
  - Строил сетку `100×100` точек (`meshgrid`).
  - Для каждой колонки сетки рассчитывал матрицу Грама и предсказания, записывал в `vals`.
  - Вырисовывал контур уровня `0` командой `plt.contour`.

### 4. Подбор гиперпараметров

- Реализовал функцию `dataset3Params(X, y, Xval, yval)`, перебирающую
  ```python
  values = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]
  ```
- На каждом сочетании `C` и `sigma`:
  1. Строил матрицу Грама для обучения и валидации.
  2. Обучал SVM с `kernel='precomputed'`.
  3. Считал ошибку на `Xval, yval` как `np.mean(preds != yval)`.
- Выбрал `C=1` и `σ=0.3` как оптимальные (минимальная ошибка).

## Результаты

- **RBF-SVM** разделил сложный набор `ex3data2.mat` без ошибок, граница сгладилась под форму данных.
- **Оптимизация** на `ex3data3.mat` дала лучшие параметры `C=1` и `σ=0.3`

## Выводы

1. Линейное ядро подходит для простых наборов, но не справляется с нелинейными данными.
2. Гауссовское ядро гибко и позволяет строить сложные границы.
3. Параметр `C` регулирует баланс между жёстким разделением и допуском ошибок, `σ` — радиус влияния каждого примера.
4. Кросс-валидация по сетке значений — надёжный метод поиска оптимальных гиперпараметров.

---

**Автор:** Алексей

**Дата:** 2025-06-16

